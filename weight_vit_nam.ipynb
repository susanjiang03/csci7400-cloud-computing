{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "025045d5-acd1-4a80-9778-9ac732298384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "from torchview import draw_graph\n",
    "from IPython.display import display\n",
    "import sympy as sp\n",
    "from datetime import datetime\n",
    "import time\n",
    "sp.init_printing(use_latex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e97529-c41a-47dd-a5ff-962693853207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get constant variables from environment\n",
    "BUCKET_NAME = \"csc7400-deepsight\"\n",
    "N = os.environ.get(\"SIZE\", None)\n",
    "BATCH_SIZE = os.environ.get(\"BATCH_SIZE\", 50)\n",
    "VAL_SPLIT = os.environ.get(\"VAL_SPLIT\", 0.2)\n",
    "NUM_WORKERS = os.environ.get(\"NUM_WORKERS\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861bebb1-7bb9-4bbb-af87-22181ec8f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 val_split=VAL_SPLIT,\n",
    "                 num_workers=NUM_WORKERS,\n",
    "                 location=\"~/datasets\",\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.batch_size = batch_size\n",
    "        self.val_split = val_split\n",
    "        self.num_workers = num_workers\n",
    "        self.location = location\n",
    "        self.input_shape = None\n",
    "        self.output_shape = None\n",
    "        self.data_train = None\n",
    "        self.data_val = None\n",
    "        self.data_test = None\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if (stage == 'fit' or \\\n",
    "                stage == 'validate') and \\\n",
    "                not (self.data_train and self.data_val):\n",
    "            start_time = time.perf_counter()\n",
    "            training_dataset = torchvision.datasets.CIFAR10(root=self.location, download=True, train=True)\n",
    "            end_time = time.perf_counter()\n",
    "            elapsed_time = round(end_time - start_time, 3)\n",
    "            print(f\" Elapsed time of set training_dataset: {elapsed_time} seconds\")\n",
    "            # CIFAR10\n",
    "            start_time = time.perf_counter()\n",
    "            x_train = training_dataset.data.transpose((0, 3, 1, 2))[:N]\n",
    "            end_time = time.perf_counter()\n",
    "            elapsed_time = round(end_time - start_time, 3)\n",
    "            print(f\" Elapsed time of set x_train: {elapsed_time} seconds\")\n",
    "            # x_train - time com\n",
    "            y_train = np.array(training_dataset.targets)[:N]\n",
    "            self.input_shape = x_train.shape[1:]\n",
    "            self.output_shape = (len(np.unique(y_train)),)\n",
    "            rng = np.random.default_rng()\n",
    "            permutation = rng.permutation(x_train.shape[0])\n",
    "            split_point = int(x_train.shape[0]*(1.0-self.val_split))\n",
    "            self.data_train = list(zip(torch.Tensor(x_train[permutation[:split_point]]).to(torch.float32),\n",
    "                                       torch.Tensor(y_train[permutation[:split_point]]).to(torch.long)))\n",
    "            self.data_val = list(zip(torch.Tensor(x_train[permutation[split_point:]]).to(torch.float32),\n",
    "                                     torch.Tensor(y_train[permutation[split_point:]]).to(torch.long)))\n",
    "        if (stage == 'test' or \\\n",
    "                stage == 'predict') and \\\n",
    "                not self.data_test:\n",
    "            testing_dataset = torchvision.datasets.CIFAR10(root=self.location, download=True, train=False)\n",
    "            x_test = testing_dataset.data.transpose((0, 3, 1, 2))[:N]\n",
    "            y_test = np.array(testing_dataset.targets)[:N]\n",
    "            self.input_shape = x_test.shape[1:]\n",
    "            self.output_shape = (len(np.unique(y_test)),)\n",
    "            self.data_test = list(zip(torch.Tensor(x_test).to(torch.float32),\n",
    "                                      torch.Tensor(y_test).to(torch.long)))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.data_train,\n",
    "                                           batch_size=self.batch_size,\n",
    "                                           num_workers=self.num_workers,\n",
    "                                           shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.data_val,\n",
    "                                           batch_size=self.batch_size,\n",
    "                                           num_workers=self.num_workers,\n",
    "                                           shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.data_test,\n",
    "                                           batch_size=self.batch_size,\n",
    "                                           num_workers=self.num_workers,\n",
    "                                           shuffle=False)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.data_test,\n",
    "                                           batch_size=self.batch_size,\n",
    "                                           num_workers=self.num_workers,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13325a79-cc4b-4bf6-ac11-a274e5223d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Elapsed time of set training_dataset: 0.862 seconds\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'N' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data_module \u001b[38;5;241m=\u001b[39m CIFAR10DataModule(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdata_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m dl \u001b[38;5;241m=\u001b[39m data_module\u001b[38;5;241m.\u001b[39mval_dataloader()\n",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m, in \u001b[0;36mCIFAR10DataModule.setup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# CIFAR10\u001b[39;00m\n\u001b[1;32m     29\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mN\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(training_dataset\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n\u001b[1;32m     32\u001b[0m x_train \u001b[38;5;241m=\u001b[39m training_dataset\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))[:N]\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'N' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "data_module = CIFAR10DataModule(batch_size=20)\n",
    "data_module.setup('fit')\n",
    "dl = data_module.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a2ec11-0a46-444d-94d8-00f9d7692db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d4ea56-2b5b-468f-a3e8-1c958eaa7700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinePositionEmbedding(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 max_wavelength=10000.0,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_wavelength = torch.Tensor([max_wavelength])\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_shape = x.shape\n",
    "        seq_length = x.shape[-2]\n",
    "        hidden_size = x.shape[-1]\n",
    "        position = torch.arange(seq_length).type_as(x)\n",
    "        min_freq = (1 / self.max_wavelength).type_as(x)\n",
    "        timescales = torch.pow(\n",
    "            min_freq,\n",
    "            (2 * (torch.arange(hidden_size) // 2)).type_as(x)\n",
    "            / torch.Tensor([hidden_size]).type_as(x)\n",
    "        )\n",
    "        angles = torch.unsqueeze(position, 1) * torch.unsqueeze(timescales, 0)\n",
    "        cos_mask = (torch.arange(hidden_size) % 2).type_as(x)\n",
    "        sin_mask = 1 - cos_mask\n",
    "        positional_encodings = (\n",
    "            torch.sin(angles) * sin_mask + torch.cos(angles) * cos_mask\n",
    "        )\n",
    "        return torch.broadcast_to(positional_encodings, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292c7f95-dbfb-41b6-94d8-1f0ab84f3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 latent_size=64,\n",
    "                 num_heads=4,\n",
    "                 dropout=0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(latent_size)\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(latent_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.activation = torch.nn.GELU()\n",
    "        self.linear = torch.nn.Linear(latent_size,\n",
    "                                      latent_size)\n",
    "        self.mha = torch.nn.MultiheadAttention(latent_size,\n",
    "                                               num_heads,\n",
    "                                               dropout=dropout,\n",
    "                                               batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.layer_norm1(y)\n",
    "        y = self.mha(y, y, y)[0]\n",
    "        x = y = x + y\n",
    "        y = self.layer_norm2(y)\n",
    "        y = self.linear(y)\n",
    "        y = self.dropout(y)\n",
    "        y = self.activation(y)\n",
    "        return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d09faa-3984-41e1-a8fc-8f216571509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trainable Module (Abstract Base Class)\n",
    "class LightningBoilerplate(pl.LightningModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        # This is the contructor, where we typically make\n",
    "        # layer objects using provided arguments.\n",
    "        super().__init__(**kwargs)  # Call the super class constructor\n",
    "\n",
    "    def predict_step(self, predict_batch, batch_idx):\n",
    "        x, y_true = predict_batch\n",
    "        y_pred = self.predict(x)\n",
    "        return y_pred, y_true\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y_true = train_batch\n",
    "        y_pred = self(x)\n",
    "        for metric_name, metric_function in self.network_metrics.items():\n",
    "            metric_value = metric_function(y_pred, y_true)\n",
    "            self.log('train_'+metric_name, metric_value, on_step=False, on_epoch=True)\n",
    "        loss = self.network_loss(y_pred, y_true)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y_true = val_batch\n",
    "        y_pred = self(x)\n",
    "        for metric_name, metric_function in self.network_metrics.items():\n",
    "            metric_value = metric_function(y_pred, y_true)\n",
    "            self.log('val_'+metric_name, metric_value, on_step=False, on_epoch=True)\n",
    "        loss = self.network_loss(y_pred, y_true)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y_true = test_batch\n",
    "        y_pred = self(x)\n",
    "        for metric_name, metric_function in self.network_metrics.items():\n",
    "            metric_value = metric_function(y_pred, y_true)\n",
    "            self.log('test_'+metric_name, metric_value, on_step=False, on_epoch=True)\n",
    "        loss = self.network_loss(y_pred, y_true)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285e765-e836-4c9c-a482-746a527572a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach loss, metrics, and optimizer\n",
    "class MultiClassLightningModule(LightningBoilerplate):\n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 **kwargs):\n",
    "        # This is the contructor, where we typically make\n",
    "        # layer objects using provided arguments.\n",
    "        super().__init__(**kwargs)  # Call the super class constructor\n",
    "\n",
    "        # This creates an accuracy function\n",
    "        self.network_metrics = torch.nn.ModuleDict({\n",
    "            'acc': torchmetrics.classification.Accuracy(task='multiclass',\n",
    "                                                        num_classes=num_classes)\n",
    "        })\n",
    "        # This creates a loss function\n",
    "        self.network_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        # change lr: 0.01, 0.0001\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d428d009-398b-47f2-8a91-07fcbb61e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach standardization and augmentation\n",
    "class StandardizeTransformModule(MultiClassLightningModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Source: https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html\n",
    "        # Needs to always be applied to any incoming\n",
    "        # image for this model. The Compose operation\n",
    "        # takes a list of torchvision transforms and\n",
    "        # applies them in sequential order, similar\n",
    "        # to neural layers...\n",
    "        self.standardize = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize([256]),\n",
    "            torchvision.transforms.CenterCrop([224]),\n",
    "            torchvision.transforms.Lambda(lambda x: x / 255.0),\n",
    "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        # Besides just standardization, the images can also undergo\n",
    "        # augmentation using torchvision. Again, we compose\n",
    "        # these operations together - ranges are provided for\n",
    "        # each of these augmentations.\n",
    "        self.transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.RandomAffine(degrees=(-10.0,10.0),\n",
    "                                                translate=(0.1,0.1),\n",
    "                                                scale=(0.9,1.1),\n",
    "                                                shear=(-10.0,10.0)),\n",
    "            torchvision.transforms.RandomHorizontalFlip(0.5),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.standardize(y)\n",
    "        if self.training:\n",
    "            y = self.transform(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d86f957-be04-4585-afbf-478d75b46947",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Channel_Att(torch.nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(Channel_Att, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.bn = torch.nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.bn(x)\n",
    "        weight_bn = self.bn.weight.data.abs() / torch.sum(self.bn.weight.data.abs())\n",
    "        x = torch.mul(weight_bn, x)\n",
    "        x = torch.sigmoid(x) * residual\n",
    "        return x\n",
    "\n",
    "\n",
    "class Spatial_Att(torch.nn.Module):\n",
    "    def __init__(self, num_tokens, channels):\n",
    "        super(Spatial_Att, self).__init__()\n",
    "        self.conv1d = torch.nn.Conv1d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def pixel_normalization(self, x):\n",
    "        norm = torch.norm(x, p=2, dim=2, keepdim=True)\n",
    "        return x / (norm + 1e-8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pixel_normalization(x)\n",
    "\n",
    "        avg_out = torch.mean(x, dim=2, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=2, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=2)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.sigmoid(x).permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "class NAM(torch.nn.Module):\n",
    "    def __init__(self, num_tokens, embed_dim):\n",
    "        super(NAM, self).__init__()\n",
    "        self.channel_att = Channel_Att(embed_dim)\n",
    "        self.spatial_att = Spatial_Att(num_tokens, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.channel_att(x)\n",
    "        spatial_weight = self.spatial_att(x)\n",
    "        x = x * spatial_weight\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948986d7-95a3-4bc5-9a17-5d3ccfa2496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTNetwork(StandardizeTransformModule):\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 patch_shape,\n",
    "                 output_size,\n",
    "                 latent_size=64,\n",
    "                 num_heads=4,\n",
    "                 n_layers=4,\n",
    "                 **kwargs):\n",
    "        super().__init__(num_classes=output_size, **kwargs)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # For MNIST, use this...\n",
    "        # self.normalize = torchvision.transforms.Compose([\n",
    "        #     torchvision.transforms.Lambda(lambda x: x / 255.0),\n",
    "        # ])\n",
    "        # Note that this normalization only makes sense for CIFAR!\n",
    "\n",
    "        self.patches = torch.nn.Conv2d(input_shape[1],\n",
    "                                       latent_size,\n",
    "                                       patch_shape,\n",
    "                                       patch_shape,\n",
    "                                       bias=False)\n",
    "        # self.position_embedding = torch.nn.Embedding((input_shape[-1]//patch_shape[-1])*\n",
    "        #                                              (input_shape[-2]//patch_shape[-2]),\n",
    "        #                                              latent_size)\n",
    "\n",
    "        self.position_embedding = SinePositionEmbedding()\n",
    "        self.att = NAM(num_tokens=(input_shape[-1] // patch_shape[-1]) *\n",
    "                                       (input_shape[-2] // patch_shape[-2]),\n",
    "                             embed_dim=latent_size)\n",
    "        self.transformer_blocks = torch.nn.Sequential(*[\n",
    "            TransformerBlock(latent_size=latent_size,\n",
    "                             num_heads=num_heads) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.pooling = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        self.linear = torch.nn.Linear(latent_size,\n",
    "                                      output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = super().forward(y)\n",
    "        y = self.patches(y)\n",
    "        y = y.reshape(y.shape[0:2] + (-1,)).permute(0, 2, 1)\n",
    "        # y = y + self.position_embedding(torch.arange(0,y.shape[1]).type_as(x).long())\n",
    "        y = y + self.position_embedding(y)\n",
    "        y = self.att(y)\n",
    "        y = self.transformer_blocks(y).permute(0, 2, 1)\n",
    "        y = self.pooling(y).squeeze()\n",
    "        y = self.linear(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edfa74e-7c27-4a22-a48f-8cdf8a285b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aaa959-541b-4c65-93d5-4d53f6da8732",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b81a1-982f-49f8-93d0-cf04c1dc53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_net = ViTNetwork(input_shape=batch[0].shape,\n",
    "                     patch_shape=(16, 16),\n",
    "                     output_size=data_module.output_shape[0],\n",
    "                     latent_size=64,\n",
    "                     n_layers=4)\n",
    "summary(vit_net, input_size=batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b9757-9dbe-48f6-84a8-be68d9a657ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = pl.loggers.CSVLogger(\"logs\",\n",
    "                              name=\"2024-10-10-Transformers\",\n",
    "                              version=\"vit-0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd7600-c30c-446f-9c85-c5c7da60c664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(logger=logger,\n",
    "                     max_epochs=1,\n",
    "                     # epochs\n",
    "                     enable_progress_bar=True,\n",
    "                     log_every_n_steps=0,\n",
    "                     enable_checkpointing=True,\n",
    "                     callbacks=[pl.callbacks.TQDMProgressBar(refresh_rate=50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd935db-9657-42c0-a03d-db052ed14434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Validation DataLoader 0:   0%|                                                                                                      | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Validation DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 22.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Validation DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 21.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val_acc            0.10000000149011612\n",
      "        val_loss            2.3553812503814697\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_acc': 0.10000000149011612, 'val_loss': 2.3553812503814697}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(vit_net, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7c513-c79e-401c-bff3-c436247fa387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Sanity Checking DataLoader 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training: |                                                                                                                          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training:   0%|                                                                                                                     | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 0:   0%|                                                                                                                      | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:11<00:00,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:11<00:00,  3.53it/s, v_num=it-0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Validation: |                                                                                                                        | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Validation:   0%|                                                                                                                   | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Validation DataLoader 0:   0%|                                                                                                      | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Validation DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 20.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:11<00:00,  3.37it/s, v_num=it-0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:11<00:00,  3.37it/s, v_num=it-0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:11<00:00,  3.36it/s, v_num=it-0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(vit_net, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b7551-fc45-4998-9df2-f4f1c163cdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(logger.log_dir+\"/metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3797ad-8fa3-44b9-a927-b6d43b4c7403",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results[\"epoch\"][np.logical_not(np.isnan(results[\"train_loss\"]))],\n",
    "         results[\"train_loss\"][np.logical_not(np.isnan(results[\"train_loss\"]))],\n",
    "         label=\"Training\")\n",
    "plt.plot(results[\"epoch\"][np.logical_not(np.isnan(results[\"val_loss\"]))],\n",
    "         results[\"val_loss\"][np.logical_not(np.isnan(results[\"val_loss\"]))],\n",
    "         label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"CCE Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be954e-6a69-401f-aa3a-8e538cb5e1db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(results[\"epoch\"][np.logical_not(np.isnan(results[\"train_acc\"]))],\n",
    "         results[\"train_acc\"][np.logical_not(np.isnan(results[\"train_acc\"]))],\n",
    "         label=\"Training\")\n",
    "plt.plot(results[\"epoch\"][np.logical_not(np.isnan(results[\"val_acc\"]))],\n",
    "         results[\"val_acc\"][np.logical_not(np.isnan(results[\"val_acc\"]))],\n",
    "         label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced63a9a-8812-4011-9b55-1d94f1bfc102",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "model_path = f\"/tmp/{N}_{VAL_SPLIT}vit_nam_model_weights.pth\"\n",
    "torch.save(vit_net.state_dict(), model_path)\n",
    "\n",
    "# upload to s3\n",
    "s3 = boto3.client('s3')\n",
    "output_filepath = f\"checkpoints/{N}_{VAL_SPLIT}_vit_nam_model_weights.pth\"\n",
    "s3.upload_file(model_path, BUCKET_NAME, output_filepath)\n",
    "print(\"✅ Upload model to S3！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsight",
   "language": "python",
   "name": "deepsight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
